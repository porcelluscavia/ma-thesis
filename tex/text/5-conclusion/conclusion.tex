\label{chap:conclusion}

Both in a traditional linguistic context such as dialectology and in a more recent applied context such as detecting sexist content in tweets, applying explainable machine learning techniques can be insightful.
In both tasks, many of the features with high importance scores fall into recurring groups that can be analyzed.
In the case of dialect classification, some of these groups fit in with common dialectological observations while others present patterns in the data that are not typically discussed.
In the context of tweet classification, these groups of features can be used to determine whether a model is trustworthy enough to be used in real applications.
While the attention weights for input features produce somewhat similar high-ranking results as LIME does, they fail at putting particular focus on highly distinctive features and thus produce less trustworthy insights into the model's classification process.

There is ample opportunity for continuing this work, for instance by exploring which input features with high importance scores tend to appear in incorrectly classified utterances or tweets, or by applying other kinds of explainable machine learning and comparing the results.
